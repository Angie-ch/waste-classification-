{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyONHfsb7dIE+p/DaO9Rt+2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angie-ch/waste-classification-/blob/main/waste_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mCK0vlSpbwYD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow.contrib.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import scipy\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6IhXyjicB5T",
        "outputId": "f629e378-ab5d-4f88-e5eb-6d093826d8e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect directory\n",
        "data_dir = Path('Garbage\\original_images')\n",
        "\n",
        "transformer = T.Compose([T.Resize((32, 32)), T.ToTensor()])\n",
        "dataset = ImageFolder(data_dir, transform = transformer)\n",
        "\n",
        "# display class names\n",
        "print(dataset.classes)"
      ],
      "metadata": {
        "id": "rd5UV5UGb3oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the path to the dataset\n",
        "dataset_dir = '/content/drive/MyDrive/waste dataset'\n",
        "\n",
        "# Define the path to where the split datasets will be saved\n",
        "train_dir = 'train_data'\n",
        "test_dir = 'test_data'\n",
        "\n",
        "# List of class names\n",
        "class_names = ['glass', 'plastic', 'paper', 'metal']\n",
        "\n",
        "# Create directories for train and test sets (if they do not exist)\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for each class in train and test directories (if they do not exist)\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Split the dataset for each class\n",
        "for class_name in class_names:\n",
        "    # Get the path to the subfolder inside each class\n",
        "    class_folder = os.path.join(dataset_dir, class_name)\n",
        "\n",
        "    # List subdirectories (e.g., \"paper waste\", \"glass waste\")\n",
        "    subfolders = [f for f in os.listdir(class_folder) if os.path.isdir(os.path.join(class_folder, f))]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        subfolder_path = os.path.join(class_folder, subfolder)\n",
        "\n",
        "        # Get the list of image files in the current subfolder\n",
        "        image_files = [f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))]\n",
        "\n",
        "        # Shuffle the list of image files to ensure randomness\n",
        "        random.shuffle(image_files)\n",
        "\n",
        "        # Calculate the number of images for training (70%)\n",
        "        total_images = len(image_files)\n",
        "        train_size = int(0.7 * total_images)\n",
        "\n",
        "        # Split the files into training and testing sets\n",
        "        train_images = image_files[:train_size]\n",
        "        test_images = image_files[train_size:]\n",
        "\n",
        "        # Move the training images to the train directory\n",
        "        for image in train_images:\n",
        "            src = os.path.join(subfolder_path, image)\n",
        "            dst = os.path.join(train_dir, class_name, image)\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "        # Move the testing images to the test directory\n",
        "        for image in test_images:\n",
        "            src = os.path.join(subfolder_path, image)\n",
        "            dst = os.path.join(test_dir, class_name, image)\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "        print(f\"Processed subfolder '{subfolder}' of class '{class_name}': {len(train_images)} train images, {len(test_images)} test images\")\n",
        "\n",
        "print(\"Dataset split complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48YTMIGad4Qq",
        "outputId": "12bef378-3496-4e9f-97c2-8ea64c18196b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed subfolder 'glass waste' of class 'glass': 0 train images, 0 test images\n",
            "Processed subfolder '.ipynb_checkpoints' of class 'glass': 0 train images, 0 test images\n",
            "Processed subfolder 'Plastic' of class 'plastic': 0 train images, 0 test images\n",
            "Processed subfolder 'paper waste' of class 'paper': 0 train images, 0 test images\n",
            "Processed subfolder 'metal waste' of class 'metal': 0 train images, 0 test images\n",
            "Processed subfolder '.ipynb_checkpoints' of class 'metal': 0 train images, 0 test images\n",
            "Dataset split complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TRAIN = r\"content\\drive\\myDrive\\train_data\"\n",
        "PATH_TEST = r\"content\\drive\\myDrive\\test_data\"\n",
        "class_names = ['glass', 'metal','paper','plastic']"
      ],
      "metadata": {
        "id": "drO8FZZheAis"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define image paths using os.path.join\n",
        "imagepath_paper = os.path.join('train_data', 'paper')\n",
        "graypath_paper = os.path.join('test_data', 'paper')\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(graypath_paper, exist_ok=True)\n",
        "# Check if the directory exists\n",
        "os.path.exists(imagepath_paper)  # This should return True if the path is correct\n",
        "\n",
        "print(f\"Checking path: {imagepath_paper}\")\n",
        "if os.path.exists(imagepath_paper):\n",
        "    print(\"Directory found!\")\n",
        "    file_listing = os.listdir(imagepath_paper)\n",
        "else:\n",
        "    print(f\"Directory not found: {imagepath_paper}\")\n",
        "\n",
        "# List files in the source directory\n",
        "file_listing = os.listdir(imagepath_paper)\n",
        "\n",
        "# Process each file in the directory\n",
        "for file in file_listing:\n",
        "    # Build the full path to the image file\n",
        "    file_path = os.path.join(imagepath_paper, file)\n",
        "\n",
        "    im = Image.open(file_path)\n",
        "\n",
        "    img = im.resize((32, 32))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = img.convert('L')\n",
        "\n",
        "    gray_file_path = os.path.join(graypath_paper, file)\n",
        "\n",
        "    # Save the grayscale image as JPEG\n",
        "    gray.save(gray_file_path, \"JPEG\")\n",
        "\n",
        "print(\"Image processing completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cobcJ18CCeH0",
        "outputId": "029fca54-0942-4b97-b546-3c63b9dcc098"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking path: train_data/paper\n",
            "Directory found!\n",
            "Image processing completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for training\n",
        "imagepath_paper = r\"content\\drive\\myDrive\\train_data\\paper\"\n",
        "#for testing\n",
        "graypath_paper= r\"content\\drive\\myDrive\\test_data\\paper\"\n",
        "File_listing = os.listdir(imagepath_paper)\n",
        "for file in File_listing:\n",
        "    im = Image.open(imagepath_paper + '\\\\' + file)\n",
        "    img = im.resize((32,32))\n",
        "    gray = img.convert('L')\n",
        "    gray.save(graypath_paper + '\\\\' + file, \"JPEG\")\n"
      ],
      "metadata": {
        "id": "hNcVrgQJeIJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}